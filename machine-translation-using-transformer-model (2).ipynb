{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":5193333,"sourceType":"datasetVersion","datasetId":3019642}],"dockerImageVersionId":30262,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Name: Fares Mohamed Salah\n# ID: 22011614","metadata":{}},{"cell_type":"markdown","source":"## Data Preprocessing","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries\nimport pathlib\nimport random\nimport string\nimport re\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers import TextVectorization\nfrom tensorflow.keras.layers import Bidirectional,GRU,LSTM,Embedding\nfrom tensorflow.keras.layers import Dense,MultiHeadAttention,LayerNormalization,Embedding,Dropout,Layer\nfrom tensorflow.keras import Sequential,Input\nfrom tensorflow.keras.callbacks import ModelCheckpoint\n\nfrom nltk.translate.bleu_score import sentence_bleu","metadata":{"execution":{"iopub.status.busy":"2024-12-23T19:55:44.360812Z","iopub.execute_input":"2024-12-23T19:55:44.361072Z","iopub.status.idle":"2024-12-23T19:55:50.420006Z","shell.execute_reply.started":"2024-12-23T19:55:44.361012Z","shell.execute_reply":"2024-12-23T19:55:50.419048Z"},"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Load the dataset from the provided file\ntext_file = '/kaggle/input/french/fra.txt'","metadata":{"execution":{"iopub.status.busy":"2024-12-23T19:55:50.421919Z","iopub.execute_input":"2024-12-23T19:55:50.423024Z","iopub.status.idle":"2024-12-23T19:55:50.426619Z","shell.execute_reply.started":"2024-12-23T19:55:50.422995Z","shell.execute_reply":"2024-12-23T19:55:50.425838Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"with open(text_file) as f:\n    lines = f.read().split(\"\\n\")[:-1]\n\n# Prepare text pairs (English and French sentences)\ntext_pairs = []\nfor line in lines:\n    english, french = line.split(\"\\t\")\n    french = \"[start] \" + french + \" [end]\"\n    text_pairs.append((english, french))","metadata":{"execution":{"iopub.status.busy":"2024-12-23T19:55:50.427849Z","iopub.execute_input":"2024-12-23T19:55:50.428176Z","iopub.status.idle":"2024-12-23T19:55:50.720404Z","shell.execute_reply.started":"2024-12-23T19:55:50.428144Z","shell.execute_reply":"2024-12-23T19:55:50.719699Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Display a random text pair\nimport random\nprint(random.choice(text_pairs))","metadata":{"execution":{"iopub.status.busy":"2024-12-23T19:55:50.721415Z","iopub.execute_input":"2024-12-23T19:55:50.721697Z","iopub.status.idle":"2024-12-23T19:55:50.726465Z","shell.execute_reply.started":"2024-12-23T19:55:50.721672Z","shell.execute_reply":"2024-12-23T19:55:50.725585Z"},"trusted":true},"outputs":[{"name":"stdout","text":"(\"I didn't graduate.\", \"[start] Je n'ai pas été diplômée. [end]\")\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Shuffle text pairs and split into training, validation, and test datasets\nimport random\nrandom.shuffle(text_pairs)\nnum_val_samples = int(0.15 * len(text_pairs))\nnum_train_samples = len(text_pairs) - 2 * num_val_samples\ntrain_pairs = text_pairs[:num_train_samples]\nval_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]\ntest_pairs = text_pairs[num_train_samples + num_val_samples:]","metadata":{"execution":{"iopub.status.busy":"2024-12-23T19:55:50.729481Z","iopub.execute_input":"2024-12-23T19:55:50.730103Z","iopub.status.idle":"2024-12-23T19:55:50.858149Z","shell.execute_reply.started":"2024-12-23T19:55:50.730068Z","shell.execute_reply":"2024-12-23T19:55:50.857493Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Define characters to strip from the text\nstrip_chars = string.punctuation + \"¿\"\nstrip_chars = strip_chars.replace(\"[\", \"\")\nstrip_chars = strip_chars.replace(\"]\", \"\")","metadata":{"execution":{"iopub.status.busy":"2024-12-23T19:55:50.859085Z","iopub.execute_input":"2024-12-23T19:55:50.859308Z","iopub.status.idle":"2024-12-23T19:55:50.867649Z","shell.execute_reply.started":"2024-12-23T19:55:50.859287Z","shell.execute_reply":"2024-12-23T19:55:50.863726Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Custom standardization function for text preprocessing\ndef custom_standardization(input_string):\n    lowercase = tf.strings.lower(input_string)\n    return tf.strings.regex_replace(\n        lowercase, f\"[{re.escape(strip_chars)}]\", \"\")","metadata":{"execution":{"iopub.status.busy":"2024-12-23T19:55:50.868632Z","iopub.execute_input":"2024-12-23T19:55:50.868932Z","iopub.status.idle":"2024-12-23T19:55:50.879749Z","shell.execute_reply.started":"2024-12-23T19:55:50.868899Z","shell.execute_reply":"2024-12-23T19:55:50.878860Z"},"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Set parameters for text vectorization\nvocab_size = 15000\nsequence_length = 20\n\n# Initialize TextVectorization layers for source and target languages\nsource_vectorization = TextVectorization(\n    max_tokens=vocab_size,\n    output_mode=\"int\",\n    output_sequence_length=sequence_length,\n)\ntarget_vectorization = TextVectorization(\n    max_tokens=vocab_size,\n    output_mode=\"int\",\n    output_sequence_length=sequence_length + 1,\n    standardize=custom_standardization,\n)\n\n# Adapt the vectorization layers using the training data\ntrain_english_texts = [pair[0] for pair in train_pairs]\ntrain_french_texts = [pair[1] for pair in train_pairs]\nsource_vectorization.adapt(train_english_texts)\ntarget_vectorization.adapt(train_french_texts)","metadata":{"execution":{"iopub.status.busy":"2024-12-23T19:55:50.880811Z","iopub.execute_input":"2024-12-23T19:55:50.881075Z","iopub.status.idle":"2024-12-23T19:56:04.226499Z","shell.execute_reply.started":"2024-12-23T19:55:50.881052Z","shell.execute_reply":"2024-12-23T19:56:04.225712Z"},"trusted":true},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Set batch size for training\nbatch_size = 64\n\n# Function to format dataset for training\ndef format_dataset(eng, fre):\n    eng = source_vectorization(eng)\n    fre = target_vectorization(fre)\n    return ({\n        \"english\": eng,\n        \"french\": fre[:, :-1],\n    }, fre[:, 1:])\n\n# Function to create a TensorFlow dataset from text pairs\ndef make_dataset(pairs):\n    eng_texts, fre_texts = zip(*pairs)\n    eng_texts = list(eng_texts)\n    fre_texts = list(fre_texts)\n    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, fre_texts))\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.map(format_dataset, num_parallel_calls=4)\n    return dataset.shuffle(2048).prefetch(16).cache()\n\n# Create training and validation datasets\ntrain_ds = make_dataset(train_pairs)\nval_ds = make_dataset(val_pairs)","metadata":{"execution":{"iopub.status.busy":"2024-12-23T19:56:04.227585Z","iopub.execute_input":"2024-12-23T19:56:04.227853Z","iopub.status.idle":"2024-12-23T19:56:05.434661Z","shell.execute_reply.started":"2024-12-23T19:56:04.227830Z","shell.execute_reply":"2024-12-23T19:56:05.433915Z"},"trusted":true},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"## LSTM Model","metadata":{}},{"cell_type":"markdown","source":"### Imports","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries\nimport random\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom nltk.translate.bleu_score import sentence_bleu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T19:56:05.435646Z","iopub.execute_input":"2024-12-23T19:56:05.435873Z","iopub.status.idle":"2024-12-23T19:56:05.440898Z","shell.execute_reply.started":"2024-12-23T19:56:05.435853Z","shell.execute_reply":"2024-12-23T19:56:05.439874Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"### Model Definition","metadata":{}},{"cell_type":"code","source":"# Define the LSTM model architecture\nembed_dim = 256\nlstm_units = 512\n\nencoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\nx = Embedding(vocab_size, embed_dim, mask_zero=True)(encoder_inputs)\nencoder_outputs, state_h, state_c = LSTM(lstm_units, return_state=True)(x)\nencoder_states = [state_h, state_c]\n\ndecoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"french\")\nx = Embedding(vocab_size, embed_dim, mask_zero=True)(decoder_inputs)\nx = LSTM(lstm_units, return_sequences=True, return_state=True)(x, initial_state=encoder_states)\ndecoder_outputs = Dense(vocab_size, activation=\"softmax\")(x[0])\n\nlstm_model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T19:56:05.442084Z","iopub.execute_input":"2024-12-23T19:56:05.442415Z","iopub.status.idle":"2024-12-23T19:56:07.053597Z","shell.execute_reply.started":"2024-12-23T19:56:05.442382Z","shell.execute_reply":"2024-12-23T19:56:07.052875Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"### Model Training","metadata":{}},{"cell_type":"code","source":"# Training parameters and setup\nepochs = 10\n\ncheckpoint = ModelCheckpoint(filepath='lstm_language_translation_checkpoint.hdf5', save_weights_only=True, verbose=1, monitor='val_accuracy')\n\nlstm_model.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n\n# Train the model\nlstm_model.fit(train_ds, epochs=epochs, callbacks=[checkpoint], validation_data=val_ds)\n\n# Saving model weights\nlstm_model.save_weights(\"lstm_translator.h5\")\nload_status = lstm_model.load_weights(\"lstm_translator.h5\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T19:56:07.054759Z","iopub.execute_input":"2024-12-23T19:56:07.055108Z","iopub.status.idle":"2024-12-23T20:12:07.420017Z","shell.execute_reply.started":"2024-12-23T19:56:07.055073Z","shell.execute_reply":"2024-12-23T20:12:07.419252Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/10\n1828/1828 [==============================] - 106s 53ms/step - loss: 1.6270 - accuracy: 0.4050 - val_loss: 1.3186 - val_accuracy: 0.4828\n\nEpoch 00001: saving model to lstm_language_translation_checkpoint.hdf5\nEpoch 2/10\n1828/1828 [==============================] - 95s 52ms/step - loss: 1.2241 - accuracy: 0.5250 - val_loss: 1.1187 - val_accuracy: 0.5592\n\nEpoch 00002: saving model to lstm_language_translation_checkpoint.hdf5\nEpoch 3/10\n1828/1828 [==============================] - 95s 52ms/step - loss: 1.0513 - accuracy: 0.5887 - val_loss: 1.0179 - val_accuracy: 0.6016\n\nEpoch 00003: saving model to lstm_language_translation_checkpoint.hdf5\nEpoch 4/10\n1828/1828 [==============================] - 95s 52ms/step - loss: 0.9407 - accuracy: 0.6332 - val_loss: 0.9636 - val_accuracy: 0.6250\n\nEpoch 00004: saving model to lstm_language_translation_checkpoint.hdf5\nEpoch 5/10\n1828/1828 [==============================] - 95s 52ms/step - loss: 0.8660 - accuracy: 0.6642 - val_loss: 0.9321 - val_accuracy: 0.6386\n\nEpoch 00005: saving model to lstm_language_translation_checkpoint.hdf5\nEpoch 6/10\n1828/1828 [==============================] - 95s 52ms/step - loss: 0.8034 - accuracy: 0.6880 - val_loss: 0.9124 - val_accuracy: 0.6480\n\nEpoch 00006: saving model to lstm_language_translation_checkpoint.hdf5\nEpoch 7/10\n1828/1828 [==============================] - 95s 52ms/step - loss: 0.7569 - accuracy: 0.7078 - val_loss: 0.9020 - val_accuracy: 0.6506\n\nEpoch 00007: saving model to lstm_language_translation_checkpoint.hdf5\nEpoch 8/10\n1828/1828 [==============================] - 95s 52ms/step - loss: 0.7147 - accuracy: 0.7238 - val_loss: 0.8927 - val_accuracy: 0.6544\n\nEpoch 00008: saving model to lstm_language_translation_checkpoint.hdf5\nEpoch 9/10\n1828/1828 [==============================] - 95s 52ms/step - loss: 0.6790 - accuracy: 0.7374 - val_loss: 0.8861 - val_accuracy: 0.6553\n\nEpoch 00009: saving model to lstm_language_translation_checkpoint.hdf5\nEpoch 10/10\n1828/1828 [==============================] - 95s 52ms/step - loss: 0.6490 - accuracy: 0.7480 - val_loss: 0.8804 - val_accuracy: 0.6568\n\nEpoch 00010: saving model to lstm_language_translation_checkpoint.hdf5\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"### Model Evaluation","metadata":{}},{"cell_type":"code","source":"# Evaluate the model on the test data\nfra_vocab = target_vectorization.get_vocabulary()\nfra_index_lookup = dict(zip(range(len(fra_vocab)), fra_vocab))\nmax_decoded_sentence_length = 20\n\ndef decode_sequence_lstm(input_sentence):\n    tokenized_input_sentence = source_vectorization([input_sentence])\n    decoded_sentence = \"[start]\"\n    for i in range(max_decoded_sentence_length):\n        tokenized_target_sentence = target_vectorization([decoded_sentence])[:, :-1]\n        predictions = lstm_model([tokenized_input_sentence, tokenized_target_sentence])\n        sampled_token_index = np.argmax(predictions[0, i, :])\n        sampled_token = fra_index_lookup[sampled_token_index]\n        decoded_sentence += \" \" + sampled_token\n        if sampled_token == \"[end]\":\n            break\n    return decoded_sentence\n\ntest_eng_texts = [pair[0] for pair in test_pairs]\nfor _ in range(5):\n    input_sentence = random.choice(test_eng_texts)\n    print(\"-\")\n    print(input_sentence)\n    print(decode_sequence_lstm(input_sentence))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T20:12:07.421132Z","iopub.execute_input":"2024-12-23T20:12:07.421395Z","iopub.status.idle":"2024-12-23T20:12:07.990096Z","shell.execute_reply.started":"2024-12-23T20:12:07.421371Z","shell.execute_reply":"2024-12-23T20:12:07.989166Z"}},"outputs":[{"name":"stdout","text":"-\nI'm not an expert.\n[start] je ne suis pas une comme téléphone [end]\n-\nDo you know what I mean?\n[start] saistu ce que je veux dire [end]\n-\nPlease go up to the third floor.\n[start] sil te plaît va à pied au [UNK] [end]\n-\nTom has calmed down.\n[start] tom a la temps [end]\n-\nShe accused me of being a liar.\n[start] elle ma accusé dêtre une menteuse [end]\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# Evaluation using the BLEU score\ntest_eng_texts = [pair[0] for pair in test_pairs]\ntest_fra_texts = [pair[1] for pair in test_pairs]\nscore = 0\nbleu = 0\nfor i in range(20):\n    candidate = decode_sequence_lstm(test_eng_texts[i])\n    reference = test_fra_texts[i].lower()\n    print(candidate, reference)\n    score = sentence_bleu(reference, candidate, weights=(1, 0, 0, 0))\n    bleu += score\n    print(f\"Score: {score}\")\nprint(f\"\\nBLEU score : {round(bleu, 2)}/20\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T20:12:07.992938Z","iopub.execute_input":"2024-12-23T20:12:07.993216Z","iopub.status.idle":"2024-12-23T20:12:10.745397Z","shell.execute_reply.started":"2024-12-23T20:12:07.993190Z","shell.execute_reply":"2024-12-23T20:12:10.744523Z"}},"outputs":[{"name":"stdout","text":"[start] que ten êtesvous ce déjeuner [end] [start] qu’avez-vous mangé au déjeuner aujourd'hui ? [end]\nScore: 0.38095238095238093\n[start] il faut que vous le voir ditesmoi cela [end] [start] tu dois le voir pour le croire. [end]\nScore: 0.30769230769230765\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \nThe hypothesis contains 0 counts of 2-gram overlaps.\nTherefore the BLEU score evaluates to 0, independently of\nhow many N-gram overlaps of lower order it contains.\nConsider using lower n-gram order or use SmoothingFunction()\n  warnings.warn(_msg)\n/opt/conda/lib/python3.7/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \nThe hypothesis contains 0 counts of 3-gram overlaps.\nTherefore the BLEU score evaluates to 0, independently of\nhow many N-gram overlaps of lower order it contains.\nConsider using lower n-gram order or use SmoothingFunction()\n  warnings.warn(_msg)\n/opt/conda/lib/python3.7/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \nThe hypothesis contains 0 counts of 4-gram overlaps.\nTherefore the BLEU score evaluates to 0, independently of\nhow many N-gram overlaps of lower order it contains.\nConsider using lower n-gram order or use SmoothingFunction()\n  warnings.warn(_msg)\n","output_type":"stream"},{"name":"stdout","text":"[start] sil vous plaît prends passer cette soin [end] [start] je te prie de m'aider à descendre ceci. [end]\nScore: 0.24528301886792456\n[start] on ma volé ma montre [end] [start] ma montre a été volée. [end]\nScore: 0.4411764705882353\n[start] je narrive pas à croire que vous ne soyez pas au moins disposé à manger une carte de arriver [end] [start] je n'arrive pas à croire que tu ne sois pas tout au moins disposé à envisager d'autres possibilités. [end]\nScore: 0.20754716981132076\n[start] il doit venir chez moi ce soir [end] [start] il va venir chez moi ce soir. [end]\nScore: 0.4090909090909091\n[start] tout le monde nous rend à nouveau sujet [end] [start] tout le monde se moque de nous. [end]\nScore: 0.2641509433962264\n[start] le ma foule me rendit à propos de ton [UNK] [end] [start] l'agent de police me fit signe de m'arrêter. [end]\nScore: 0.2807017543859649\n[start] laissezmoi vous montrer quelque chose de vraiment être votre sujet [end] [start] laissez-moi vous montrer quelque chose de vraiment super. [end]\nScore: 0.25\n[start] le problème est que je ne peux pas aller à lécole il est à manger douche [end] [start] le problème est que mon fils ne veut pas aller à l'école. [end]\nScore: 0.2558139534883721\n[start] je suis surpris quils a fait cela [end] [start] je suis agréablement surpris par cela. [end]\nScore: 0.34042553191489366\n[start] elle a commencé à pleurer [end] [start] elle s'est mise à pleurer. [end]\nScore: 0.38461538461538464\n[start] Êtesvous [UNK] [end] [start] es-tu effrayé ? [end]\nScore: 0.39285714285714285\n[start] arrête je vous prie [end] [start] cessez, je vous prie ! [end]\nScore: 0.48484848484848486\n[start] je ne suis pas [UNK] de quoi que ce soit [end] [start] je ne comprends rien. [end]\nScore: 0.2777777777777778\n[start] je vous ai demandé la nuit dernière [end] [start] je t'ai demandé hier soir. [end]\nScore: 0.3061224489795918\n[start] il a bu la musique en ta [end] [start] il partagea la pomme en deux. [end]\nScore: 0.3684210526315789\n[start] ces livres ne vous en mesure que je sois au réunion [end] [start] ces ouvrages valent le coup d'être lus au moins une fois. [end]\nScore: 0.26153846153846155\n[start] cest le plus grand de tous les autres [end] [start] c'est le plus grand de tous les garçons. [end]\nScore: 0.3137254901960784\n[start] jai une douleur dans ma poche [end] [start] j'ai un chat dans la gorge. [end]\nScore: 0.3953488372093023\n\nBLEU score : 6.57/20\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"## Transformer Model","metadata":{}},{"cell_type":"markdown","source":"### Imports","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries\nimport random\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers import Embedding, Dense, LayerNormalization, MultiHeadAttention, Dropout\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom nltk.translate.bleu_score import sentence_bleu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T20:12:10.746393Z","iopub.execute_input":"2024-12-23T20:12:10.746688Z","iopub.status.idle":"2024-12-23T20:12:10.751565Z","shell.execute_reply.started":"2024-12-23T20:12:10.746663Z","shell.execute_reply":"2024-12-23T20:12:10.750653Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"### Model Definition","metadata":{}},{"cell_type":"code","source":"# Define the Transformer Encoder class\nclass TransformerEncoder(Layer):\n    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n        super(TransformerEncoder, self).__init__(**kwargs)\n        self.embed_dim = embed_dim\n        self.dense_dim = dense_dim\n        self.num_heads = num_heads\n        self.attention = MultiHeadAttention(\n            num_heads=num_heads, key_dim=embed_dim\n        )\n        self.dense_proj = keras.Sequential(\n            [Dense(dense_dim, activation=\"relu\"), Dense(embed_dim),]\n        )\n        self.layernorm_1 = LayerNormalization()\n        self.layernorm_2 = LayerNormalization()\n        self.supports_masking = True\n\n    def call(self, inputs, mask=None):\n        if mask is not None:\n            padding_mask = tf.cast(mask[:, tf.newaxis, tf.newaxis, :], dtype=\"int32\")\n        attention_output = self.attention(\n            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\n        )\n\n        proj_input = self.layernorm_1(inputs + attention_output)\n        proj_output = self.dense_proj(proj_input)\n        return self.layernorm_2(proj_input + proj_output)\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\n            \"embed_dim\": self.embed_dim,\n            \"dense_dim\": self.dense_dim,\n            \"num_heads\": self.num_heads,\n        })\n        return config","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T20:12:10.752589Z","iopub.execute_input":"2024-12-23T20:12:10.752845Z","iopub.status.idle":"2024-12-23T20:12:10.765143Z","shell.execute_reply.started":"2024-12-23T20:12:10.752814Z","shell.execute_reply":"2024-12-23T20:12:10.764362Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# Define the Transformer Decoder class\nclass TransformerDecoder(Layer):\n    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = embed_dim\n        self.dense_dim = dense_dim\n        self.num_heads = num_heads\n        self.attention_1 = MultiHeadAttention(\n            num_heads=num_heads, key_dim=embed_dim)\n        self.attention_2 = MultiHeadAttention(\n            num_heads=num_heads, key_dim=embed_dim)\n        self.dense_proj = keras.Sequential(\n            [Dense(dense_dim, activation=\"relu\"),\n             Dense(embed_dim),]\n        )\n        self.layernorm_1 = LayerNormalization()\n        self.layernorm_2 = LayerNormalization()\n        self.layernorm_3 = LayerNormalization()\n        self.supports_masking = True\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\n            \"embed_dim\": self.embed_dim,\n            \"num_heads\": self.num_heads,\n            \"dense_dim\": self.dense_dim,\n        })\n        return config\n\n    def get_causal_attention_mask(self, inputs):\n        input_shape = tf.shape(inputs)\n        batch_size, sequence_length = input_shape[0], input_shape[1]\n        i = tf.range(sequence_length)[:, tf.newaxis]\n        j = tf.range(sequence_length)\n        mask = tf.cast(i >= j, dtype=\"int32\")\n        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n        mult = tf.concat(\n            [tf.expand_dims(batch_size, -1),\n             tf.constant([1, 1], dtype=tf.int32)], axis=0)\n        return tf.tile(mask, mult)\n\n    def call(self, inputs, encoder_outputs, mask=None):\n        causal_mask = self.get_causal_attention_mask(inputs)\n        if mask is not None:\n            padding_mask = tf.cast(\n                mask[:, tf.newaxis, :], dtype=\"int32\")\n            padding_mask = tf.minimum(padding_mask, causal_mask)\n        attention_output_1 = self.attention_1(\n            query=inputs,\n            value=inputs,\n            key=inputs,\n            attention_mask=causal_mask)\n        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n        attention_output_2 = self.attention_2(\n            query=attention_output_1,\n            value=encoder_outputs,\n            key=encoder_outputs,\n            attention_mask=padding_mask,\n        )\n        attention_output_2 = self.layernorm_2(\n            attention_output_1 + attention_output_2)\n        proj_output = self.dense_proj(attention_output_2)\n        return self.layernorm_3(attention_output_2 + proj_output)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T20:12:10.766248Z","iopub.execute_input":"2024-12-23T20:12:10.766688Z","iopub.status.idle":"2024-12-23T20:12:10.783512Z","shell.execute_reply.started":"2024-12-23T20:12:10.766655Z","shell.execute_reply":"2024-12-23T20:12:10.782654Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# Define the Positional Embedding class\nclass PositionalEmbedding(Layer):\n    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n        super().__init__(**kwargs)\n        self.token_embeddings = Embedding(\n            input_dim=input_dim, output_dim=output_dim)\n        self.position_embeddings = Embedding(input_dim=input_dim, output_dim=output_dim)\n        self.sequence_length = sequence_length\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n\n    def call(self, inputs):\n        length = tf.shape(inputs)[-1]\n        positions = tf.range(start=0, limit=length, delta=1)\n        embedded_tokens = self.token_embeddings(inputs)\n        embedded_positions = self.position_embeddings(positions)\n        return embedded_tokens + embedded_positions\n\n    def compute_mask(self, inputs, mask=None):\n        return tf.math.not_equal(inputs, 0)\n\n    def get_config(self):\n        config = super(PositionalEmbedding, self).get_config()\n        config.update({\n            \"output_dim\": self.output_dim,\n            \"sequence_length\": self.sequence_length,\n            \"input_dim\": self.input_dim,\n        })\n        return config","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T20:12:10.784615Z","iopub.execute_input":"2024-12-23T20:12:10.784933Z","iopub.status.idle":"2024-12-23T20:12:10.800895Z","shell.execute_reply.started":"2024-12-23T20:12:10.784901Z","shell.execute_reply":"2024-12-23T20:12:10.799955Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# Build the Transformer model architecture\nembed_dim = 256\ndense_dim = 2048\nnum_heads = 8\n\nencoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\nx = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\nencoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n\ndecoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"french\")\nx = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\nx = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)\nx = Dropout(0.5)(x)\ndecoder_outputs = Dense(vocab_size, activation=\"softmax\")(x)\ntransformer_model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T20:12:10.801836Z","iopub.execute_input":"2024-12-23T20:12:10.802076Z","iopub.status.idle":"2024-12-23T20:12:11.397891Z","shell.execute_reply.started":"2024-12-23T20:12:10.802055Z","shell.execute_reply":"2024-12-23T20:12:11.397164Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"### Model Training","metadata":{}},{"cell_type":"code","source":"# Training parameters and setup\nepochs = 10\n\ncheckpoint = ModelCheckpoint(filepath='transformer_language_translation_checkpoint.hdf5', save_weights_only=True, verbose=1, monitor='val_accuracy')\n\ntransformer_model.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n\n# Train the model\ntransformer_model.fit(train_ds, epochs=epochs, callbacks=[checkpoint], validation_data=val_ds)\n\n# Saving model weights\ntransformer_model.save_weights(\"transformer_translator.h5\")\nload_status = transformer_model.load_weights(\"transformer_translator.h5\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T20:12:11.399180Z","iopub.execute_input":"2024-12-23T20:12:11.399987Z","iopub.status.idle":"2024-12-23T20:32:04.602727Z","shell.execute_reply.started":"2024-12-23T20:12:11.399950Z","shell.execute_reply":"2024-12-23T20:32:04.601935Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/10\n1828/1828 [==============================] - 122s 65ms/step - loss: 1.5894 - accuracy: 0.4636 - val_loss: 1.2368 - val_accuracy: 0.5590\n\nEpoch 00001: saving model to transformer_language_translation_checkpoint.hdf5\nEpoch 2/10\n1828/1828 [==============================] - 118s 65ms/step - loss: 1.2367 - accuracy: 0.5771 - val_loss: 1.0649 - val_accuracy: 0.6175\n\nEpoch 00002: saving model to transformer_language_translation_checkpoint.hdf5\nEpoch 3/10\n1828/1828 [==============================] - 118s 65ms/step - loss: 1.0978 - accuracy: 0.6204 - val_loss: 1.0118 - val_accuracy: 0.6411\n\nEpoch 00003: saving model to transformer_language_translation_checkpoint.hdf5\nEpoch 4/10\n1828/1828 [==============================] - 119s 65ms/step - loss: 1.0391 - accuracy: 0.6461 - val_loss: 0.9862 - val_accuracy: 0.6532\n\nEpoch 00004: saving model to transformer_language_translation_checkpoint.hdf5\nEpoch 5/10\n1828/1828 [==============================] - 119s 65ms/step - loss: 1.0083 - accuracy: 0.6626 - val_loss: 0.9723 - val_accuracy: 0.6629\n\nEpoch 00005: saving model to transformer_language_translation_checkpoint.hdf5\nEpoch 6/10\n1828/1828 [==============================] - 119s 65ms/step - loss: 0.9866 - accuracy: 0.6759 - val_loss: 0.9590 - val_accuracy: 0.6695\n\nEpoch 00006: saving model to transformer_language_translation_checkpoint.hdf5\nEpoch 7/10\n1828/1828 [==============================] - 119s 65ms/step - loss: 0.9688 - accuracy: 0.6863 - val_loss: 0.9585 - val_accuracy: 0.6761\n\nEpoch 00007: saving model to transformer_language_translation_checkpoint.hdf5\nEpoch 8/10\n1828/1828 [==============================] - 119s 65ms/step - loss: 0.9521 - accuracy: 0.6957 - val_loss: 0.9607 - val_accuracy: 0.6755\n\nEpoch 00008: saving model to transformer_language_translation_checkpoint.hdf5\nEpoch 9/10\n1828/1828 [==============================] - 119s 65ms/step - loss: 0.9360 - accuracy: 0.7037 - val_loss: 0.9493 - val_accuracy: 0.6820\n\nEpoch 00009: saving model to transformer_language_translation_checkpoint.hdf5\nEpoch 10/10\n1828/1828 [==============================] - 119s 65ms/step - loss: 0.9200 - accuracy: 0.7104 - val_loss: 0.9533 - val_accuracy: 0.6832\n\nEpoch 00010: saving model to transformer_language_translation_checkpoint.hdf5\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"### Model Evaluation","metadata":{}},{"cell_type":"code","source":"# Evaluate the model on the test data\nfra_vocab = target_vectorization.get_vocabulary()\nfra_index_lookup = dict(zip(range(len(fra_vocab)), fra_vocab))\nmax_decoded_sentence_length = 20\n\ndef decode_sequence_transformer(input_sentence):\n    tokenized_input_sentence = source_vectorization([input_sentence])\n    decoded_sentence = \"[start]\"\n    for i in range(max_decoded_sentence_length):\n        tokenized_target_sentence = target_vectorization([decoded_sentence])[:, :-1]\n        predictions = transformer_model([tokenized_input_sentence, tokenized_target_sentence])\n        sampled_token_index = np.argmax(predictions[0, i, :])\n        sampled_token = fra_index_lookup[sampled_token_index]\n        decoded_sentence += \" \" + sampled_token\n        if sampled_token == \"[end]\":\n            break\n    return decoded_sentence\n\ntest_eng_texts = [pair[0] for pair in test_pairs]\nfor _ in range(5):\n    input_sentence = random.choice(test_eng_texts)\n    print(\"-\")\n    print(input_sentence)\n    print(decode_sequence_transformer(input_sentence))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T20:32:04.603797Z","iopub.execute_input":"2024-12-23T20:32:04.604042Z","iopub.status.idle":"2024-12-23T20:32:05.676896Z","shell.execute_reply.started":"2024-12-23T20:32:04.604020Z","shell.execute_reply":"2024-12-23T20:32:05.676129Z"}},"outputs":[{"name":"stdout","text":"-\nShe was a bridesmaid at the wedding.\n[start] elle était [UNK] en [UNK] au mariage [end]\n-\nI gave my seat to the old lady.\n[start] jai donné ma place à vieux [end]\n-\nThe students were ill at ease before the exam.\n[start] les étudiants se [UNK] à lintérieur de lexamen [end]\n-\nIsn't that true?\n[start] pas cela vrai [end]\n-\nI didn't want to spend any more time alone.\n[start] je ne voulais plus passer un moment plus seul [end]\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# Evaluation using the BLEU score\ntest_eng_texts = [pair[0] for pair in test_pairs]\ntest_fra_texts = [pair[1] for pair in test_pairs]\nscore = 0\nbleu = 0\nfor i in range(20):\n    candidate = decode_sequence_transformer(test_eng_texts[i])\n    reference = test_fra_texts[i].lower()\n    print(candidate, reference)\n    score = sentence_bleu(reference, candidate, weights=(1, 0, 0, 0))\n    bleu += score\n    print(f\"Score: {score}\")\nprint(f\"\\nBLEU score : {round(bleu, 2)}/20\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T20:32:05.677929Z","iopub.execute_input":"2024-12-23T20:32:05.678223Z","iopub.status.idle":"2024-12-23T20:32:09.754815Z","shell.execute_reply.started":"2024-12-23T20:32:05.678197Z","shell.execute_reply":"2024-12-23T20:32:09.754023Z"}},"outputs":[{"name":"stdout","text":"[start] questce que vous a mangé le déjeuner aujourdhui [end] [start] qu’avez-vous mangé au déjeuner aujourd'hui ? [end]\nScore: 0.32786885245901637\n[start] vous devez le voir à croire [end] [start] tu dois le voir pour le croire. [end]\nScore: 0.3902439024390244\n[start] je vous prie de maider ceci [end] [start] je te prie de m'aider à descendre ceci. [end]\nScore: 0.3658536585365854\n[start] ma fait [UNK] ma montre [end] [start] ma montre a été volée. [end]\nScore: 0.32432432432432434\n[start] je ne peux pas croire que vous ne soyez pas au moins [UNK] à [UNK] [end] [start] je n'arrive pas à croire que tu ne sois pas tout au moins disposé à envisager d'autres possibilités. [end]\nScore: 0.25\n[start] il doit venir à ma maison ce soir [end] [start] il va venir chez moi ce soir. [end]\nScore: 0.34042553191489366\n[start] tout le monde [UNK] [end] [start] tout le monde se moque de nous. [end]\nScore: 0.42424242424242425\n[start] [UNK] de ma part à se [UNK] [end] [start] l'agent de police me fit signe de m'arrêter. [end]\nScore: 0.2926829268292683\n[start] laissemoi te montrer quelque chose de très [UNK] [end] [start] laissez-moi vous montrer quelque chose de vraiment super. [end]\nScore: 0.2903225806451613\n[start] le problème cest que mon fils ne veut pas aller à lécole [end] [start] le problème est que mon fils ne veut pas aller à l'école. [end]\nScore: 0.34285714285714286\n[start] je lai [UNK] par [end] [start] je suis agréablement surpris par cela. [end]\nScore: 0.4666666666666667\n[start] elle se [UNK] [end] [start] elle s'est mise à pleurer. [end]\nScore: 0.4074074074074074\n[start] Êtesvous [UNK] [end] [start] es-tu effrayé ? [end]\nScore: 0.39285714285714285\n[start] arrête sil vous plaît [end] [start] cessez, je vous prie ! [end]\nScore: 0.42857142857142855\n[start] je ne suis rien [UNK] [end] [start] je ne comprends rien. [end]\nScore: 0.34285714285714286\n[start] je vous ai demandé la nuit dernière [end] [start] je t'ai demandé hier soir. [end]\nScore: 0.3061224489795918\n[start] il a coupé la [UNK] en moitié [end] [start] il partagea la pomme en deux. [end]\nScore: 0.37209302325581395\n[start] ces livres sont en train de moins [UNK] [end] [start] ces ouvrages valent le coup d'être lus au moins une fois. [end]\nScore: 0.3018867924528302\n[start] il est le plus gros de tous les garçons [end] [start] c'est le plus grand de tous les garçons. [end]\nScore: 0.3018867924528302\n[start] jai une [UNK] [end] [start] j'ai un chat dans la gorge. [end]\nScore: 0.48148148148148145\n\nBLEU score : 7.15/20\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"## Pre-trained model from Hugging Face","metadata":{}},{"cell_type":"markdown","source":"### Model Loading","metadata":{}},{"cell_type":"code","source":"from transformers import MarianMTModel, MarianTokenizer\n\n# Load pretrained model and tokenizer for English to French translation\nmodel_name = \"Helsinki-NLP/opus-mt-en-fr\"\ntokenizer = MarianTokenizer.from_pretrained(model_name)\nmodel = MarianMTModel.from_pretrained(model_name)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T20:32:27.288080Z","iopub.execute_input":"2024-12-23T20:32:27.288920Z","iopub.status.idle":"2024-12-23T20:32:30.925224Z","shell.execute_reply.started":"2024-12-23T20:32:27.288882Z","shell.execute_reply":"2024-12-23T20:32:30.924501Z"}},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"### Inference","metadata":{}},{"cell_type":"code","source":"def translate_with_huggingface(input_sentence):\n    # Tokenize the input sentence\n    tokenized_input_sentence = tokenizer.encode(input_sentence, return_tensors=\"pt\", padding=True, truncation=True)\n    \n    # Get the prediction from the model\n    translated = model.generate(tokenized_input_sentence, max_length=50, num_beams=4, early_stopping=True)\n    \n    # Decode the translated sentence\n    translated_sentence = tokenizer.decode(translated[0], skip_special_tokens=True)\n    return translated_sentence\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T20:32:35.111969Z","iopub.execute_input":"2024-12-23T20:32:35.113062Z","iopub.status.idle":"2024-12-23T20:32:35.118062Z","shell.execute_reply.started":"2024-12-23T20:32:35.113024Z","shell.execute_reply":"2024-12-23T20:32:35.117044Z"}},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":"### Test and Comparison","metadata":{}},{"cell_type":"code","source":"# Test with both models\ntest_eng_texts = [pair[0] for pair in test_pairs]\nfor input_sentence in test_eng_texts[:5]:  # Adjust to the desired number of sentences\n    print(\"Original English sentence:\", input_sentence)\n    \n    # Translate with LSTM model\n    LSTM_translation = decode_sequence_lstm(input_sentence)\n    print(\"LSTM Translation:\", LSTM_translation)\n\n    # Translate with Transformer model\n    transformer_translation = decode_sequence_transformer(input_sentence)\n    print(\"Transformer Translation:\", transformer_translation)\n    \n    # Translate with Hugging Face model\n    hf_translation = translate_with_huggingface(input_sentence)\n    print(\"Hugging Face Translation:\", hf_translation)\n    print(\"-\" * 50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T20:33:33.825041Z","iopub.execute_input":"2024-12-23T20:33:33.825277Z","iopub.status.idle":"2024-12-23T20:33:33.838047Z","shell.execute_reply.started":"2024-12-23T20:33:33.825255Z","shell.execute_reply":"2024-12-23T20:33:33.836725Z"}},"outputs":[{"name":"stdout","text":"Original English sentence: I have a proposal.\nLSTM Translation: [start] jai une proposition [end]\nTransformer Translation: [start]                    \nHugging Face Translation: J'ai une proposition.\n--------------------------------------------------\nOriginal English sentence: I've got all the friends I need.\nLSTM Translation: [start] jai tous mes amis [end]\nTransformer Translation: [start]                    \nHugging Face Translation: J'ai tous les amis dont j'ai besoin.\n--------------------------------------------------\nOriginal English sentence: Where were you?\nLSTM Translation: [start] où êtesvous [end]\nTransformer Translation: [start]                    \nHugging Face Translation: Où étais-tu?\n--------------------------------------------------\nOriginal English sentence: We had fun with them.\nLSTM Translation: [start] nous nous [UNK] [end]\nTransformer Translation: [start]                    \nHugging Face Translation: On s'est amusés avec eux.\n--------------------------------------------------\nOriginal English sentence: I want you to talk to me.\nLSTM Translation: [start] je veux que tu [UNK] [end]\nTransformer Translation: [start]                    \nHugging Face Translation: Je veux que tu me parles.\n--------------------------------------------------\n","output_type":"stream"}],"execution_count":25}]}
